{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa51485-8420-47d7-b06c-9a8d2ca1e75b",
   "metadata": {},
   "source": [
    "So this is a working convolutional SNN with the Architecture proposed by that one paper I suggested.\n",
    "This is neither optimized code nor is it compatible with our data yet but I thought it is a reasonable starting place.\n",
    "\n",
    "So this Network receives the raw data as inputs not spikes. It seems easiest to just put in the spectogram data (what we called voltage) into the LIF neurons instead of using the poisson encoding first. (poisson encoding was at least useful for us to see if we can generate sensible spikes from the data).  \n",
    "\n",
    "Right now there is a convolution on the input data itself, I would start out by trying to avoid and only convolve in the spike domain, but we'll see. I'm also unsere whether we should give the input timestep by timestep or not. With the setup right now, it expects it all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa60ccfd-ae43-474d-b21d-0caf14a7354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 17:08:47.896882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7046a269-e4fe-4117-91ac-78cfb05794a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Spiking Neural Network class\n",
    "class CSNN(nn.Module):\n",
    "    def __init__(self, T=8, spike_grad=surrogate.ATan(), threshold=1.0, num_class=10):\n",
    "        super(CSNN, self).__init__()\n",
    "\n",
    "        self.T = T  #time steps for temporal integration\n",
    "\n",
    "        # Define the layers, right now I hardcoded the shapes\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.lif1 = neuron.LIFNode(surrogate_function=spike_grad, v_threshold=threshold)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.lif2 = neuron.LIFNode(surrogate_function=spike_grad, v_threshold=threshold)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 128) # Adjusted input size for 2 poolings\n",
    "        self.lif3 = neuron.LIFNode(surrogate_function=spike_grad, v_threshold=threshold)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.lif4 = neuron.LIFNode(surrogate_function=spike_grad, v_threshold=threshold)\n",
    "        self.fc3 = nn.Linear(64, num_class)\n",
    "        self.lif5 = neuron.LIFNode(surrogate_function=spike_grad, v_threshold=threshold)\n",
    "\n",
    "        #batch norm and dropout improve test accuracy\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions and pooling without the time loop, here we do 1 convolution before spike encoding\n",
    "        # the order I've seen being used is conv, batchnorm, LIF, pool\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.pool(self.lif1(x)) \n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = self.pool(self.lif2(x)) \n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = x.view(-1, 16 * 5 * 5)  # flatten fc layers\n",
    "        \n",
    "        # Now apply the time loop to LIF neurons in the fully connected layers\n",
    "        mem_fc1 = torch.zeros_like(self.fc1(x)) # Initialize membrane potential\n",
    "        mem_fc2 = torch.zeros_like(self.fc2(mem_fc1))\n",
    "        mem_fc3 = torch.zeros_like(self.fc3(mem_fc2))\n",
    "\n",
    "        for step in range(self.T):\n",
    "            mem_fc1 = self.lif3(self.fc1(x))\n",
    "            mem_fc2 = self.lif4(self.fc2(mem_fc1))\n",
    "            #x = self.dropout2(mem_fc2)\n",
    "            mem_fc3 = self.lif5(self.fc3(mem_fc2))\n",
    "        \n",
    "        return mem_fc3  # Return the final membrane potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a346a0-dbc3-4fa3-ba31-0395a2319b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.7680\n",
      "Epoch [2/10], Loss: 1.6347\n",
      "Epoch [3/10], Loss: 1.6232\n",
      "Epoch [4/10], Loss: 1.6159\n",
      "Epoch [5/10], Loss: 1.6099\n",
      "Epoch [6/10], Loss: 1.6064\n",
      "Epoch [7/10], Loss: 1.6045\n",
      "Epoch [8/10], Loss: 1.6009\n",
      "Epoch [9/10], Loss: 1.5982\n",
      "Epoch [10/10], Loss: 1.5973\n",
      "Accuracy on the test set: 76.42%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001 # 0.001 worked well on my tests\n",
    "num_epochs = 10 # only 10 right now so it doesn't take forever to load\n",
    "time_steps = 8  # Time steps for temporal integration, might need to increase it\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss and optimizer\n",
    "model = CSNN(T=time_steps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        functional.reset_net(model)  # we have to reset after each batch\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        functional.reset_net(model)  \n",
    "        \n",
    "print(f'Accuracy on the test set: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
