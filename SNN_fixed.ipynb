{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "#from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#data = load_dataset('google/speech_commands', 'v0.01', split=['train','test'], trust_remote_code=True)\n",
    "train_set = torchaudio.datasets.SPEECHCOMMANDS(root='C:/Users/wasse/hello', download=True, subset='training')\n",
    "test_set = torchaudio.datasets.SPEECHCOMMANDS(root='C:/Users/wasse/hello', download=True, subset='testing')\n",
    "\n",
    "max_length = 16000  # or your desired length\n",
    "# Assume `labels` contains the string labels from your dataset\n",
    "all_labels = [item[2] for item in train_set] \n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_length = 16000\n",
    "    waveforms, labels = zip(*[(item[0].squeeze()[:max_length], item[2]) for item in batch])\n",
    "\n",
    "    # Ensure each waveform is 1D before padding\n",
    "    waveforms = [waveform if waveform.ndim == 1 else waveform.mean(dim=0) for waveform in waveforms]\n",
    "\n",
    "    #print(\"Shapes before padding:\", [w.shape for w in waveforms])  # Add this line\n",
    "\n",
    "    waveforms_padded = pad_sequence(waveforms, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = label_encoder.transform(labels)  # Encode the labels as integers\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    #print('done')\n",
    "    return waveforms_padded, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal.windows import exponential, gaussian\n",
    "from scipy.signal import square, ShortTimeFFT\n",
    "\n",
    "sample_rate=16000\n",
    "g_std = 10      # standard deviation for Gaussian window in samples\n",
    "win_size = 40   # window size in samples\n",
    "win_gauss = gaussian(win_size, std=g_std, sym=True)  # symmetric Gaussian wind.\n",
    "SFT = ShortTimeFFT(win_gauss, hop=2, fs=sample_rate, mfft=2000, scale_to='psd')\n",
    "batch_size = 64\n",
    "num_samples = 16000\n",
    "\n",
    "duration = num_samples / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import stft\n",
    "import librosa\n",
    "\n",
    "\"\"\"\n",
    "def mel_spectrogram(audio, sample_rate, n_mels=128, f_min=0, f_max=None):\n",
    "  if f_max is None:\n",
    "    f_max = sample_rate / 2\n",
    "  _, _, spectrogram = stft(audio, nperseg=512, noverlap=256, fs=sample_rate)\n",
    "  #print(\"spectrogram: \", spectrogram.shape)\n",
    "  # mel_spectrogram = mel(spectrogram, sr=sample_rate, n_mels=n_mels, fmin=f_min, fmax=f_max)\n",
    "  mel_spectrogram = mel(spectrogram, sr=sample_rate, n_mels=n_mels, fmin=f_min, fmax=f_max)\n",
    "  return mel_spectrogram\n",
    "\"\"\"\n",
    "\n",
    "def spectrogram(audio, sample_rate, n_mels=128, f_min=0, f_max=None):\n",
    "  if f_max is None:\n",
    "    f_max = sample_rate / 2\n",
    "  _, _, spectrogram = stft(audio, nperseg=512, noverlap=256, fs=sample_rate)\n",
    "  return spectrogram\n",
    "\n",
    "\n",
    "def mel(spectrogram, sr=44100, n_mels=128, fmin=0, fmax=None):\n",
    "  return librosa.feature.melspectrogram(S=spectrogram, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "\n",
    "class RBFNetwork:\n",
    "  def __init__(self, input_dim, num_centers, sigma):\n",
    "    self.centers = np.random.rand(num_centers, input_dim)  # Initialize centers randomly\n",
    "    #print(\"centers: \", self.centers.shape)\n",
    "    self.sigma = sigma\n",
    "\n",
    "  def rbf(self, x):\n",
    "    #print(\"x: \", x.shape)\n",
    "    #print(\"centers: \", self.centers.shape)\n",
    "    # return np.exp(-np.linalg.norm(x - self.centers, axis=1) ** 2 / (2 * self.sigma ** 2))\n",
    "    def compute_distances(xi):\n",
    "            # xi - self.centers creates a new array where each center is subtracted from xi\n",
    "            # np.linalg.norm(..., axis=1) computes the norm along the axis of the centers\n",
    "            return np.linalg.norm(xi - self.centers, axis=1)\n",
    "    norms = np.apply_along_axis(compute_distances, 1, x)\n",
    "    return np.exp(- norms ** 2 / (2 * self.sigma ** 2))\n",
    "\n",
    "  def predict(self, X):\n",
    "    #print(\"## predict ##\")\n",
    "    #print(\"X: \", X.shape)\n",
    "    y = self.rbf(X)\n",
    "    #print(\"y: \", y.shape)\n",
    "    # normalize to 0 - 1 along the batch dimension\n",
    "    y = (y - np.min(y, axis=0)) / (np.max(y, axis=0) - np.min(y, axis=0))\n",
    "    return y\n",
    "\n",
    "def rbf_encode_audio(audio, sample_rate, SFT, n_mels=128, num_rbf=160, sigma=1.0):\n",
    "  spec = spectrogram(audio, sample_rate, n_mels)\n",
    "  spec = np.abs(spec)\n",
    "  # mel_spec = SFT.spectrogram(audio)\n",
    "  #print(\"shape mel_spec: \", mel_spec.shape)\n",
    "  rbf_network = RBFNetwork(spec.shape[0], num_rbf//10, sigma)\n",
    "  rbf_activations = rbf_network.predict(spec.T)  # transpose to get the batch dimension first\n",
    "  return rbf_activations, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# real spectograms\\ndef encode_to_spikes(data, sr, tau_m=20.0, R=1.0, V_th=1.0, V_reset=0.0):\\n    batch_size = data.size(0)\\n    num_features = 128\\n    num_time_steps = 64\\n    specs = torch.zeros(batch_size, num_features, num_time_steps, device=data.device)\\n    for i in range(batch_size):\\n        #rbf_activations, mel_spec = rbf_encode_audio(data[i], sr, SFT=SFT)\\n        spec = mel_spectrogram(data[i],sr)\\n        specs[i] = torch.from_numpy(spec)\\n\\n    return specs\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import utils as sutils\n",
    "from snntorch.functional import quant\n",
    "\n",
    "class customSNet(nn.Module):\n",
    "    def __init__(self, num_steps, beta, threshold=1.0, spike_grad=snn.surrogate.fast_sigmoid(slope=25), num_class=10):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        self.fc1 = nn.Linear(448, 128)\n",
    "        #self.fc1 = nn.Linear(896, 128)  # use 6720 for real spektogram, 896 for spikes or rbf activity\n",
    "        #self.fc1 = nn.Linear(6720, 128) # use 6720 for real spektogram, 896 for spikes or rbf activity\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        self.fc3 = nn.Linear(64, num_class)\n",
    "        self.lif5 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        batch_size_curr = x.shape[0]\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "        mem5 = self.lif5.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk5_rec = []\n",
    "        mem5_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            #print('x0', x.shape)\n",
    "            cur1 = self.pool(self.conv1(x))\n",
    "            #cur1 = self.conv1(x)\n",
    "            #print('x1', x.shape)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            #print('x2', spk1.shape, mem1.shape)\n",
    "            cur2 = self.pool(self.conv2(spk1))\n",
    "            #cur2 = self.conv2(spk1)\n",
    "            #print('x3', cur2.shape)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            #print('x4', spk2.shape, mem2.shape)\n",
    "            cur3 = self.fc1(spk2.view(batch_size_curr, -1))\n",
    "            #print('x5', cur3.shape)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            #print('x6', spk3.shape, mem3.shape)\n",
    "            cur4 = self.fc2(spk3)\n",
    "            #print('x7', cur4.shape)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            #print('x8', spk4.shape, mem4.shape)\n",
    "            cur5 = self.fc3(spk4)\n",
    "            #print('x9', cur5.shape)\n",
    "            spk5, mem5 = self.lif5(cur5, mem5)\n",
    "            #print('x10', spk5.shape, mem5.shape)\n",
    "\n",
    "            spk5_rec.append(spk5)\n",
    "            mem5_rec.append(mem5)\n",
    "\n",
    "        return torch.stack(spk5_rec), torch.stack(mem5_rec)\n",
    "\n",
    "# poisson spikes from rbf activity\n",
    "def encode_to_spikes(data, sr, tau_m=20.0, R=1.0, V_th=1.0, V_reset=0.0):\n",
    "    batch_size = data.size(0)\n",
    "    num_features = 16\n",
    "    num_time_steps = 64\n",
    "    spikes = torch.zeros(batch_size, num_features, num_time_steps, device=data.device)\n",
    "    rbfs = []\n",
    "    for i in range(batch_size):\n",
    "        rbf_activations, mel_spec = rbf_encode_audio(data[i], sr, SFT=SFT, num_rbf=160)\n",
    "        rbfs.append(rbf_activations)\n",
    "        spike_prob_scale = 1.7\n",
    "        rbf_activations_traversed = rbf_activations.T\n",
    "        spik_probs = rbf_activations_traversed / np.max(rbf_activations_traversed, axis=1, keepdims=True) * spike_prob_scale\n",
    "        spike_trains = np.random.poisson(spik_probs[...] * duration, size=rbf_activations_traversed.shape)\n",
    "        spike_trains = np.clip(spike_trains, 0, 1)\n",
    "        spikes[i] = torch.from_numpy(spike_trains)\n",
    "\n",
    "    return spikes\n",
    "\"\"\"\n",
    "# pseudo spectogram of rbf activity\n",
    "def encode_to_spikes(data, sr, tau_m=20.0, R=1.0, V_th=1.0, V_reset=0.0):\n",
    "    batch_size = data.size(0)\n",
    "    num_features = 25\n",
    "    num_time_steps = 64\n",
    "    rbfs = torch.zeros(batch_size, num_features, num_time_steps, device=data.device)\n",
    "    for i in range(batch_size):\n",
    "        rbf_activations, mel_spec = rbf_encode_audio(data[i], sr, SFT=SFT)\n",
    "        rbf_activations_traversed = rbf_activations.T\n",
    "        rbfs[i] = torch.from_numpy(rbf_activations_traversed)\n",
    "\n",
    "    return rbfs\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# real spectograms\n",
    "def encode_to_spikes(data, sr, tau_m=20.0, R=1.0, V_th=1.0, V_reset=0.0):\n",
    "    batch_size = data.size(0)\n",
    "    num_features = 128\n",
    "    num_time_steps = 64\n",
    "    specs = torch.zeros(batch_size, num_features, num_time_steps, device=data.device)\n",
    "    for i in range(batch_size):\n",
    "        #rbf_activations, mel_spec = rbf_encode_audio(data[i], sr, SFT=SFT)\n",
    "        spec = mel_spectrogram(data[i],sr)\n",
    "        specs[i] = torch.from_numpy(spec)\n",
    "\n",
    "    return specs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     loss_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(mem_rec[step], labels)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Gradient calculation + weight update\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Store loss history for future plotting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\function.py:291\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# old versio with bug fixed and dimensions set to 16 to fit with tuning curves\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from snntorch import functional as SF\n",
    "\n",
    "\n",
    "num_classes = 35\n",
    "num_steps = 20\n",
    "model = customSNet(num_steps = num_steps, beta = 0.9, threshold=1.0, spike_grad=snn.surrogate.fast_sigmoid(slope=25), num_class=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "sr = 16000  # Number of time steps to simulate\n",
    "batch_size = 64\n",
    "train_loss_hist = []\n",
    "train_accu_hist = []\n",
    "train_accu_hist_temp = []\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "iterCount = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        spikes = encode_to_spikes(inputs, sr)\n",
    "        spike_input = spikes.unsqueeze(1)\n",
    "\n",
    "        model.train()\n",
    "        spk_rec, mem_rec = model(spike_input)\n",
    "        labels = labels.long()\n",
    "        loss_val = torch.zeros((1), dtype=torch.float)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss_fn(mem_rec[step], labels)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        avg_loss = loss_val.item()/len(train_loader)\n",
    "        train_loss_hist.append(loss_val.item())\n",
    "        acc = SF.accuracy_rate(spk_rec, labels)\n",
    "        acc2 = SF.accuracy_temporal(spk_rec, labels)\n",
    "        train_accu_hist.append(acc)\n",
    "        train_accu_hist_temp.append(acc2)\n",
    "        iterCount +=1\n",
    "    print(f' Epoch: {epoch} | Train Loss: {train_loss_hist[-1]:.3f} | Avg Loss: {avg_loss:.3f} | Accuracy: {train_accu_hist[-1]:.3f} | Accuracy: {train_accu_hist_temp[-1]:.3f} | Iteration: {iterCount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     94\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output_mean, labels)\n\u001b[1;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     97\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m spike_input\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\wasse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\function.py:291\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#updated version with batch norm and decaying learning rate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "class customSNet(nn.Module):\n",
    "    def __init__(self, num_steps, beta, threshold=1.0, spike_grad=snn.surrogate.fast_sigmoid(slope=25), num_class=35):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(6)  # Batch Norm after conv1\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(16)  # Batch Norm after conv2\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        \n",
    "        self.fc1 = nn.Linear(448, 128)  # 448 for rbf activations or spikes\n",
    "        self.bn3 = nn.BatchNorm1d(128)  # Batch Norm after fc1\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)  # Batch Norm after fc2\n",
    "        self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, num_class)\n",
    "        self.lif5 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size_curr = x.shape[0]\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "        mem5 = self.lif5.init_leaky()\n",
    "\n",
    "        spk5_rec = []\n",
    "        mem5_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.pool(self.bn1(self.conv1(x)))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            \n",
    "            cur2 = self.pool(self.bn2(self.conv2(spk1)))\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            \n",
    "            cur3 = self.bn3(self.fc1(spk2.view(batch_size_curr, -1)))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            \n",
    "            cur4 = self.bn4(self.fc2(spk3))\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            \n",
    "            cur5 = self.fc3(spk4)\n",
    "            spk5, mem5 = self.lif5(cur5, mem5)\n",
    "\n",
    "            spk5_rec.append(spk5)\n",
    "            mem5_rec.append(mem5)\n",
    "\n",
    "        return torch.stack(spk5_rec), torch.stack(mem5_rec)\n",
    "\n",
    "# Hyperparameters\n",
    "num_steps = 20\n",
    "beta = 0.9\n",
    "num_class = 35\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10  # Adjust based on your needs\n",
    "\n",
    "# Initialize model, loss, optimizer, and scheduler\n",
    "model = customSNet(num_steps=num_steps, beta=beta, num_class=num_class)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "\n",
    "# Example Training Loop with Validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, labels in train_loader:\n",
    "        spikes = encode_to_spikes(data, sr)\n",
    "        spike_input = spikes.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(spike_input)\n",
    "        # Aggregate outputs over time steps, e.g., mean\n",
    "        output_mean = output.mean(dim=0)\n",
    "        labels = labels.long()\n",
    "        loss = loss_fn(output_mean, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * spike_input.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            spikes = encode_to_spikes(data, sr)\n",
    "            spike_input = spikes.unsqueeze(1)\n",
    "            output, _ = model(spike_input)\n",
    "            output_mean = output.mean(dim=0)\n",
    "            preds = output_mean.argmax(dim=1)\n",
    "            labels = labels.long()\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Learning Rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
